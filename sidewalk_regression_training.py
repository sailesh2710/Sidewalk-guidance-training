# -*- coding: utf-8 -*-
"""sidewalk_regression_training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y_iAyRj3mr5zOUGOFFf-7yrJy6By0epR

### Image Regression with TensorFlow Hub: Training, Evaluation, and Deployment

This Colab demonstrates how to train an **image regression model** using a pre-trained TensorFlow Hub model. The specific task involves **assessing severity levels of sidewalk damage** based on image data. The goal is to predict a continuous severity value in the range `[1, 4]`.

#### Key Features:
1. **Custom Image Regression Model**: Train a regression head on top of a pre-trained feature extractor.
2. **Explicit Image-Label Mapping**: Ensure images and labels are correctly paired.
3. **Severity Range Transformation**: Scale predictions to align with the true range `[1, 4]`.
4. **End-to-End Workflow**:
   - Preprocess data
   - Train and evaluate the model
   - Visualize predictions
   - Export to SavedModel and TFLite formats for deployment

# STEP 1: Mount Drive and install libraries and import packages

This step ensures that the environment is ready for building, training, and evaluating the model. It involves importing required libraries, configuring the model, and initializing global variables.

#### **1.1 Import Libraries and Mount Drive**
- Import necessary libraries for deep learning (`tensorflow`, `tensorflow_hub`), data manipulation (`pandas`, `numpy`), and visualization (`matplotlib`).
- Mount Google Drive to access datasets and save model outputs like checkpoints and logs.
"""

!pip install python-docx

import itertools
import os
import PIL
import tf_keras
import matplotlib.pylab as plt
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
import re
from docx import Document
from docx.shared import Inches
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay



from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.losses import MeanAbsoluteError, MeanAbsolutePercentageError
from tensorflow.keras.callbacks import History
from typing import Iterator, List, Union, Tuple, Any
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""### **1.2 Define Model Configuration**

*   Define the model architecture using pre-trained TensorFlow Hub models like EfficientNetV2.
*   Map model names to their corresponding TensorFlow Hub URLs and set input image size configurations.
"""

#@title

model_name = "efficientnetv2-b0" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']

model_handle_map = {
  "efficientnetv2-s": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2",
  "efficientnetv2-m": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2",
  "efficientnetv2-l": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2",
  "efficientnetv2-s-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2",
  "efficientnetv2-m-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2",
  "efficientnetv2-l-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2",
  "efficientnetv2-xl-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2",
  "efficientnetv2-b0-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2",
  "efficientnetv2-b1-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2",
  "efficientnetv2-b2-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2",
  "efficientnetv2-b3-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2",
  "efficientnetv2-s-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2",
  "efficientnetv2-m-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2",
  "efficientnetv2-l-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2",
  "efficientnetv2-xl-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2",
  "efficientnetv2-b0-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2",
  "efficientnetv2-b1-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2",
  "efficientnetv2-b2-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2",
  "efficientnetv2-b3-21k-ft1k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2",
  "efficientnetv2-b0": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2",
  "efficientnetv2-b1": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2",
  "efficientnetv2-b2": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2",
  "efficientnetv2-b3": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2",
  "efficientnet_b0": "https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1",
  "efficientnet_b1": "https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1",
  "efficientnet_b2": "https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1",
  "efficientnet_b3": "https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1",
  "efficientnet_b4": "https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1",
  "efficientnet_b5": "https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1",
  "efficientnet_b6": "https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1",
  "efficientnet_b7": "https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1",
  "bit_s-r50x1": "https://tfhub.dev/google/bit/s-r50x1/1",
  "inception_v3": "https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4",
  "inception_resnet_v2": "https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4",
  "resnet_v1_50": "https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4",
  "resnet_v1_101": "https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4",
  "resnet_v1_152": "https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4",
  "resnet_v2_50": "https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4",
  "resnet_v2_101": "https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4",
  "resnet_v2_152": "https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4",
  "nasnet_large": "https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4",
  "nasnet_mobile": "https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4",
  "pnasnet_large": "https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4",
  "mobilenet_v2_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4",
  "mobilenet_v2_130_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4",
  "mobilenet_v2_140_224": "https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4",
  "mobilenet_v3_small_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5",
  "mobilenet_v3_small_075_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5",
  "mobilenet_v3_large_100_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5",
  "mobilenet_v3_large_075_224": "https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5",
}

model_image_size_map = {
  "efficientnetv2-s": 384,
  "efficientnetv2-m": 480,
  "efficientnetv2-l": 480,
  "efficientnetv2-b0": 224,
  "efficientnetv2-b1": 240,
  "efficientnetv2-b2": 260,
  "efficientnetv2-b3": 300,
  "efficientnetv2-s-21k": 384,
  "efficientnetv2-m-21k": 480,
  "efficientnetv2-l-21k": 480,
  "efficientnetv2-xl-21k": 512,
  "efficientnetv2-b0-21k": 224,
  "efficientnetv2-b1-21k": 240,
  "efficientnetv2-b2-21k": 260,
  "efficientnetv2-b3-21k": 300,
  "efficientnetv2-s-21k-ft1k": 384,
  "efficientnetv2-m-21k-ft1k": 480,
  "efficientnetv2-l-21k-ft1k": 480,
  "efficientnetv2-xl-21k-ft1k": 512,
  "efficientnetv2-b0-21k-ft1k": 224,
  "efficientnetv2-b1-21k-ft1k": 240,
  "efficientnetv2-b2-21k-ft1k": 260,
  "efficientnetv2-b3-21k-ft1k": 300,
  "efficientnet_b0": 224,
  "efficientnet_b1": 240,
  "efficientnet_b2": 260,
  "efficientnet_b3": 300,
  "efficientnet_b4": 380,
  "efficientnet_b5": 456,
  "efficientnet_b6": 528,
  "efficientnet_b7": 600,
  "inception_v3": 299,
  "inception_resnet_v2": 299,
  "nasnet_large": 331,
  "pnasnet_large": 331,
}

model_handle = model_handle_map.get(model_name)
pixels = model_image_size_map.get(model_name, 224)

print(f"Selected model: {model_name} : {model_handle}")

IMAGE_SIZE = (pixels, pixels)
print(f"Input size {IMAGE_SIZE}")

BATCH_SIZE = 16 #@param {type:"integer"}
print(f"Batch size {BATCH_SIZE}")

"""# Step 2: Set up the Dataset
This section focuses on preparing the dataset for training, validation, and testing. It includes reading data, splitting it into subsets, and converting it into numpy arrays for further processing.

#### **2.1: Read the regression.csv File**
Load the dataset containing image paths and their corresponding severity values using pandas DataFrame.
"""

# Set directories
i_dir = "drive/MyDrive/Regression/images"
image_dir = "drive/MyDrive/Regression/images"
main_dir = "drive/MyDrive/Regression"
log_dir = "drive/MyDrive/Regression/tensorboard/"
saved_model_path = main_dir + "/saved_model"

# Load data file
dataFile = main_dir + "/regression.csv"
!cat $dataFile

# Read CSV into DataFrame
pandasDataFrame = pd.read_csv(dataFile)
print(pandasDataFrame)
print(pandasDataFrame['image'])
print(pandasDataFrame['severity'])

# Update image paths
pandasDataFrame["image"] = image_dir + "/" + pandasDataFrame["image"]
for x in pandasDataFrame["image"]:
    print(x)

"""#### **2.2 split the Dataset**
Divide the dataset into training, validation, and testing sets using train_test_split() from sklearn



"""

# Split percentages
testAndValid_percent = 0.2  # Percentage for test + validation
test_valid_split_percent = 0.5  # Percentage split between test and validation

# Function to split data
def split_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    train, val_test = train_test_split(df, test_size=testAndValid_percent, random_state=1)
    val, test = train_test_split(val_test, test_size=test_valid_split_percent, random_state=1)

    print("Train shape:", train.shape)
    print("Validation shape:", val.shape)
    print("Test shape:", test.shape)
    return train, val, test

# Split the data
train, val, test = split_data(pandasDataFrame)

# Print split details
print("Train, Validation, Test sizes:", len(train), len(val), len(test))

"""#### **STEP 2.3 Prepare Image and Severity Data**

###### **Step 2.3.1: Load and Resize Images**
Convert image paths to numpy arrays and resize them to the expected input size.
"""

# Initialize dictionaries
trainImages, testImages, validationImages = {}, {}, {}
trainSeverities, testSeverities, validationSeverities = {}, {}, {}

# Load training images
for i, row in train.iterrows():
    trainSeverities[i] = row["severity"]
    img = row["image"]
    trainImages[i] = np.array(PIL.Image.open(img).resize(IMAGE_SIZE))

# Load testing images
for i, row in test.iterrows():
    testSeverities[i] = row["severity"]
    img = row["image"]
    testImages[i] = np.array(PIL.Image.open(img).resize(IMAGE_SIZE))

# Load validation images
for i, row in val.iterrows():
    validationSeverities[i] = row["severity"]
    img = row["image"]
    validationImages[i] = np.array(PIL.Image.open(img).resize(IMAGE_SIZE))

"""###### **Step 2.3.2: Convert Images to NumPy Arrays**
Convert the loaded images into numpy arrays for input to the model
"""

# Convert images to numpy arrays
trainImageData = np.array([img for img in trainImages.values()]).astype(np.float32)
validationImageData = np.array([img for img in validationImages.values()]).astype(np.float32)
testImageData = np.array([img for img in testImages.values()]).astype(np.float32)

"""###### **Step 2.3.3: Convert Severities to NumPy Arrays**
Prepare severity values as numpy arrays for use in training, validation, and testing.
"""

# Convert severities to numpy arrays
trainSeverity = np.fromiter(trainSeverities.values(), dtype=np.float32)
validationSeverity = np.fromiter(validationSeverities.values(), dtype=np.float32)
testSeverity = np.fromiter(testSeverities.values(), dtype=np.float32)

"""# **STEP 3: Defining the model**

#### **Step 3.1: Define the Model Architecture**
The regression model is built by adding a single dense output layer on top of a pre-trained feature extractor from TensorFlow Hub. This ensures the output range is constrained to [1, 4] using a sigmoid activation function followed by scaling.
"""

# Enable fine-tuning
do_fine_tuning = True  # @param {type:"boolean"}

# Define the model-building function
def build_model(model_handle, do_fine_tuning):
    """
    Builds the regression model with output restricted to [1, 4].
    """
    model = tf_keras.Sequential([
        tf_keras.layers.InputLayer(input_shape=(224, 224, 3)),  # Input layer
        hub.KerasLayer(model_handle, trainable=do_fine_tuning),  # Pre-trained model
        tf_keras.layers.Dropout(rate=0.2),  # Dropout for regularization
        tf_keras.layers.Dense(1, activation="sigmoid"),  # Output restricted to [0, 1]
        tf_keras.layers.Lambda(lambda x: x * 3 + 1, name="scaled_output")  # Scale to [1, 4]
    ])
    return model

# Build the model
model = build_model(model_handle, do_fine_tuning=True)

# Display model summary
model.summary()

"""#  **STEP 4: Training the model for regression**

#### **Step 4.1: Compile the Model**
Compile the model using mean_absolute_error as the loss function, which is well-suited for regression tasks. The optimizer is set to SGD with a learning rate of 0.0005.
"""

# Compile the model
model.compile(
    optimizer=tf_keras.optimizers.SGD(learning_rate=0.0005, momentum=0.9),
    loss=tf_keras.losses.MeanAbsoluteError(),  # Use MAE for regression
    metrics=["mae", "mse"]  # Additional metrics for evaluation
)

"""#### **Step 4.2: Setup Callbacks**
To save the best model weights during training and enable TensorBoard for monitoring.
"""

# Set up checkpoint directory
checkpoint_dir = "drive/MyDrive/Regression/checkpoints/Run2/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5"
mc = tf_keras.callbacks.ModelCheckpoint(filepath=checkpoint_dir, save_best_only=True, verbose=1, save_freq='epoch')   # save_weights_only=True


# Set up TensorBoard callback
tensorboard_callback = tf_keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# List checkpoints for reference
!ls "drive/MyDrive/Regression/checkpoints"

"""#### **Step 4.3: Train the Model**
Train the model using the training data and validate with the validation set.
"""

# Steps per epoch calculations
steps_per_epoch = max(1, len(trainImageData) // BATCH_SIZE)
validation_steps = max(1, len(validationImageData) // BATCH_SIZE)
epochs = 1000  # You can increase this to 1000 for better results

# Validation data tuple
validationData = (validationImageData, validationSeverity)

# Train the model
hist = model.fit(
    x=trainImageData,
    y=trainSeverity,
    epochs=epochs,
    steps_per_epoch=steps_per_epoch,
    validation_data=validationData,
    validation_steps=validation_steps,
    callbacks=[mc, tensorboard_callback]
).history

"""#### **Step 4.4: Plot Training and Validation Loss**
Visualize the loss during training and validation.
"""

# Plot loss curves
plt.figure()
plt.ylabel("Loss (training and validation)")
plt.xlabel("Training Steps")
plt.plot(hist["loss"], color='orange', label="Training Loss")
plt.plot(hist["val_loss"], color='blue', label="Validation Loss")
plt.legend()
plt.show()

"""# Step 5: Evaluate and Test the Model

#### **Step 5.1: Evaluate the Model**
Evaluate the model on the test dataset.
"""

# Evaluate the model
evaluation_results = model.evaluate(testImageData, testSeverity, batch_size=BATCH_SIZE, verbose=2)

# Access the loss value (first element in the evaluation results)
loss = evaluation_results[0]

# Print the formatted result
print(f"Trained model, loss: {loss:.4f}")

"""#### **Step 5.2: Test Predictions on a Single Image**
Make predictions on a single image and compare them to the true severity.
"""

# Test on a single image
image = trainImageData[0]
true_severity = trainSeverity[0]
plt.imshow(image / 255.0)  # Normalize for display
plt.axis('off')
plt.show()

# Predict severity
prediction_scores = model.predict(np.expand_dims(image, axis=0))
predicted_severity = prediction_scores[0][0]
print(f"True Severity: {true_severity}")
print(f"Predicted Severity: {predicted_severity:.2f}")

"""#### **Step 5.3: Predict on the Entire Test Set**
Make predictions on the entire test set.
"""

# Predict on test data
predictions = model.predict(testImageData)

# Print predicted and true severities
for i, (pred, true) in enumerate(zip(predictions, testSeverity)):
    print(f"Image {i + 1}: Predicted={pred[0]:.2f}, True={true:.2f}")

"""#### **Step 5.3: Save Predictions and Visualize Results in a Word Document**
This step involves generating model predictions for the test dataset and saving the results, along with visualizations, to a Word document for reporting and analysis.
"""

# Define the output Word document path
doc_path = "drive/MyDrive/Regression/ckpt-608-detections.docx"

# Create a new Word document
document = Document()

# Add a paragraph to the document
p = document.add_paragraph()
r = p.add_run()

# Predict on the test image data
predictions = model.predict(testImageData)

# Retrieve image names and true severities from the test dataset
images_names = test["image"]
severities = test["severity"]

# Loop through the test images, predictions, and true severities
for image_name, prediction, severity in zip(images_names, predictions, severities):
    # Display the test image with prediction and severity as labels
    plt.imshow(PIL.Image.open(image_name))
    plt.xlabel(f"Prediction: {prediction[0]:.2f}\nTrue Severity: {severity}\n{image_name}")
    plt.savefig('temp.png')  # Save the image temporarily
    plt.show()

    # Add the saved image to the Word document
    r.add_picture('temp.png', width=Inches(4.0))
    r.add_break()
    r.add_break()

# Save the Word document with all the visualizations and results
document.save(doc_path)

print(f"Results saved to: {doc_path}")

"""#### **Step 5.4: Save Predictions and Truth Severities**
This step involves saving the model predictions and the actual severity values to text files for further analysis or record-keeping.
"""

# Define the directory and file paths for saving predictions and truth values
output_dir = "drive/MyDrive/Regression/testing/"
os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists

# Define file paths
prediction_file = os.path.join(output_dir, "predictions.txt")
truth_file = os.path.join(output_dir, "truth.txt")

# Save predictions
with open(prediction_file, "w") as pred_file:
    for prediction in predictions:
        pred_file.write(f"{prediction[0]:.2f}\n")  # Save the first element of each prediction

# Save severities (truth values)
with open(truth_file, "w") as truth_file:
    for severity in severities:
        truth_file.write(f"{severity}\n")  # Save each severity value

print(f"Predictions saved to: {prediction_file}")
print(f"Truth severities saved to: {truth_file}")

"""#### **Step 5.5: Save Predictions, Severities, and Image Names to a Single File**
This step combines the predictions, true severities, and corresponding image names into a single text file for comprehensive analysis.
"""

# Define the output file and ensure the directory exists
output_dir = "drive/MyDrive/Regression/testing/"
os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists

# Define the output file path
prediction_file = os.path.join(output_dir, "testing.txt")

# Save predictions, severities, and image names to the file
with open(prediction_file, "w") as txt_file3:
    for image_name, prediction, severity in zip(images_names, predictions, severities):
        line = f"{prediction[0]:.2f} , {severity} , {image_name}"  # Format the line with prediction, severity, and image name
        print(line)  # Print to console for debugging
        txt_file3.write(line + "\n")  # Write the formatted line to the file

print(f"Predictions, severities, and image names saved to: {prediction_file}")

"""#### **Step 5.6: Inferencing and Visualizing Results from a Specified Directory of Raw Images**
This step enables inferencing on raw images from a specified directory. Each image is processed, and the model's prediction is visualized alongside the image name.
"""

test_directory = "drive/MyDrive/Regression/images"
print(f"Inference folder: {test_directory}")
!ls $test_directory  # List contents of the directory

def open_images(inference_folder: str) -> np.ndarray:
    """
    Loads images from a folder and prepares them for inferencing.

    Parameters
    ----------
    inference_folder : str
        Location of images for inferencing.

    Returns
    -------
    np.ndarray
        List of images as numpy arrays transformed to fit the model input specs.
    """
    images = []
    print(f"Attempting to read images in {inference_folder}")
    # Regex to check valid image file extension
    regex = r"([^\\s]+(\.(?i)(jpe?g|png|gif|bmp))$)"
    p = re.compile(regex)  # Compile the regex

    for img in os.listdir(inference_folder):
        if not re.search(p, img):
            continue  # Skip invalid files
        img_location = os.path.join(test_directory, img)
        print(f"Processing file: {img_location}")
        with PIL.Image.open(img_location) as img:
            img = img.resize(IMAGE_SIZE)  # Resize image to match model input size
            img = np.array(img)[:, :, :3]  # Ensure the image has 3 channels
            img = np.expand_dims(img, axis=0)  # Add batch dimension

        images.append(img)
    images_array = np.vstack(images)  # Combine images efficiently into a numpy array
    return images_array

# Load and process images from the directory
myTestImages = open_images(test_directory)

# Run the model predictions on the images
predictions = model.predict(myTestImages)

# Retrieve image names from the directory
images_names = os.listdir(test_directory)

# Display each image with its prediction and name
for image_name, prediction, img in zip(images_names, predictions, myTestImages):
    plt.imshow(img / 255.0)  # Normalize pixel values for display
    plt.xlabel(f"Prediction = {prediction[0]:.2f}\n{image_name}")
    plt.show()

"""#### **Step 5.7: Plotting Training and Validation Results**
This step visualizes the training and validation results using seaborn and matplotlib. The function plot_results plots Mean Absolute Percentage Error (MAPE) for both training and validation data, with a horizontal line to indicate the baseline performance.
"""

def plot_results(model_history_eff_net: History, mean_baseline: float):
    """
    This function uses seaborn and matplotlib to plot training and validation losses.
    The mean baseline is plotted as a horizontal red dashed line.

    Parameters
    ----------
    model_history_eff_net : History
        Keras History object of the model.fit() method.
    mean_baseline : float
        Result of the baseline performance (e.g., mean baseline).
    """
    # Create dictionaries for training and validation loss
    training_dict = {
        "MAPE": model_history_eff_net["mae"],  # Using MAE instead of MAPE for this example
        "type": "training",
        "model": "eff_net",
    }
    validation_dict = {
        "MAPE": model_history_eff_net["val_mae"],  # Using validation MAE
        "type": "validation",
        "model": "eff_net",
    }

    # Convert the dictionaries to pandas DataFrames
    training_df = pd.DataFrame(training_dict)
    validation_df = pd.DataFrame(validation_dict)

    # Combine the DataFrames into a long-format DataFrame
    df = pd.concat([training_df, validation_df], axis=0).reset_index()

    # Create a relational plot
    grid = sns.relplot(data=df, x=df["index"], y="MAPE", hue="type", col="model", kind="line", legend=True)
    grid.set(ylim=(0, 4))  # Adjust the y-axis range as needed
    for ax in grid.axes.flat:
        ax.axhline(
            y=mean_baseline, color="lightcoral", linestyle="dashed"
        )  # Add a horizontal line for baseline performance
        ax.set(xlabel="Epoch")
    plt.legend(labels=["eff_net", "mean_baseline"])  # Add legend
    plt.savefig("training_validation.png")  # Save the plot as an image
    plt.show()  # Display the plot

# Plot results using the function
plot_results(hist, mean_baseline=0.0)  # Replace 0.0 with an appropriate baseline value if applicable

"""# **STEP 6: Save model as Saved Model**

This step ensures that the trained model is saved in TensorFlow's SavedModel format. It provides a flexible way to save the architecture, weights, and optimizer state for future use.
"""

print("Saving model to:", saved_model_path)

# Ensure the directory exists
os.makedirs(saved_model_path, exist_ok=True)

# Save the model in TensorFlow's SavedModel format
model.save(saved_model_path, save_format='tf')  # Saving in the 'tf' format for compatibility

print(f"Model successfully saved to: {saved_model_path}")
!ls $saved_model_path

"""# **STEP 7: Save TFlite model** for deployment to mobile devices
This step converts the trained model into TensorFlow Lite (TFLite) format for deployment on mobile and IoT devices. TFLite is optimized for performance on edge devices.
"""

# Step 7: Save Model as TFLite
print("Converting and saving model to TFLite format...")

# Convert the model to TFLite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Enable TensorFlow Select for unsupported operations
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops
    tf.lite.OpsSet.SELECT_TF_OPS  # Enable TensorFlow ops
]

# Uncomment to enable quantization
# converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Perform the conversion
tflite_model = converter.convert()

# Save the TFLite model to a file
tflite_model_file = "drive/MyDrive/Regression/tfLiteModel/regression_model.tflite"
os.makedirs(os.path.dirname(tflite_model_file), exist_ok=True)

with open(tflite_model_file, "wb") as f:
    f.write(tflite_model)

print(f"TFLite model saved to: {tflite_model_file}")

"""# **STEP 8:  Run Tensorboard**
TensorBoard is a visualization tool used to monitor metrics like loss, accuracy, and other metrics during training. This step assumes logs have been created during training.
"""

# Commented out IPython magic to ensure Python compatibility.
print("Launching TensorBoard...")

# Check for any running TensorBoard processes and kill them
!lsof -i :6006 | grep tensorboard | awk '{print $2}' | xargs -r kill

# Reload the TensorBoard extension and start TensorBoard
# %reload_ext tensorboard
# %tensorboard --logdir $log_dir

"""#STEP 9: Evaluate Metrics for Regression
Evaluate the trained model on the test dataset using metrics like Mean Absolute Error (MAE). The ground truth and predictions are also loaded if needed.
"""

print("Evaluating metrics for regression...")

# Use Mean Absolute Error
mae = tf_keras.losses.MeanAbsoluteError()

# Calculate MAE for test data
mean_absolute_error_test = mae(severities, predictions).numpy()
print(f"Mean Absolute Error (MAE) on testing data: {mean_absolute_error_test}")

"""# **STEP 10: Find something similar to Confusion Matrix**

For regression problems, discretize the ground truth and predicted values into bins to generate a confusion matrix, which helps visualize the prediction accuracy in a structured format.
"""

# Discretize predictions and ground truth into bins and convert to integers
def discretize_values(values, bin_size=0.5):
    return (np.round(values / bin_size) * bin_size).astype(int)

# Discretized predictions and ground truth
discretized_predictions = discretize_values(predictions)
discretized_severities = discretize_values(severities)
# Trim predictions to match severities
discretized_predictions = discretized_predictions[:len(discretized_severities)]

# Ensure both arrays are integers
discretized_predictions = discretized_predictions.astype(int)
discretized_severities = discretized_severities.astype(int)

# Create confusion matrix
confusion_mat = confusion_matrix(discretized_severities, discretized_predictions)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat, display_labels=np.unique(discretized_severities))
disp.plot(cmap="viridis", xticks_rotation="vertical")
plt.title("Confusion Matrix of Discretized Predictions")
plt.show()

"""# **STEP 11:  Scatter plot of predicted versus truth**

A scatter plot provides a direct comparison of predictions and ground truth values.
"""

def scatter_plot_predictions_vs_truth(truth, predictions):
    predictions = predictions[:len(severities)]
    plt.scatter(truth, predictions, c='blue', alpha=0.5, label='Data Points')
    plt.plot([0, 10], [0, 10], color='red', linestyle='--', label='Ideal Line')
    plt.xlabel('Ground Truth')
    plt.ylabel('Predicted')
    plt.legend()
    plt.title('Scatter Plot of Predictions vs Ground Truth')
    plt.show()

scatter_plot_predictions_vs_truth(severities, predictions)